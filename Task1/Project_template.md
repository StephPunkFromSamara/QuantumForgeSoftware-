

# 1.Сравнение LLM для корпоративного чат-бота (английский язык)

Сравнение локальных моделей Hugging Face и облачных решений OpenAI / YandexGPT для внутренней документации.

| **Критерий**             | **Локальные модели Hugging Face (on-prem или облако)**                                                                                                 | **OpenAI ChatGPT (облако)**                                                                                                                   | **YandexGPT (облако)**                                                                                                                        |
|---------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------|
| **Качество ответов**      | Зависит от модели. Открытые модели (LLaMA, Mistral, Falcon) можно дообучить на своих документах, но «из коробки» уступают GPT-4. Дообучение повышает релевантность и точность. | Высокое качество «из коробки», особенно GPT-4. Чёткие и последовательные ответы на сложные запросы, отличная поддержка английского языка. | Оптимизирована под русский язык. Английский поддерживается, но качество ниже, чем у GPT-4. Для английских документов может уступать OpenAI. |
| **Скорость (латентность)** | Зависит от железа и размера модели. Малые/средние модели (7–13B параметров) — ~1 секунда на GPU. Большие модели (70B+) требуют нескольких секунд. Нет сетевой задержки, но нужны мощные серверы. | Быстрое облачное API, обычно 1–2 секунды. GPT-4 медленнее GPT-3.5. Зависит от нагрузки и сети. | Lite-модель быстрее, Pro — точнее. Латентность обычно 1–3 секунды. |
| **Стоимость (TCO)**       | Высокие капитальные затраты: серверы с GPU, электроэнергия, охлаждение — десятки тысяч USD/год для крупных моделей. Нет платы за запросы, но расходы на обслуживание и инженеров. При очень высокой нагрузке может быть дешевле облака. | Оплата по использованию (OPEX). GPT-4 ~ $0.06 за 1K токенов. Нет расходов на инфраструктуру. Для малых и средних нагрузок дешевле. | Оплата за 1K токенов. Lite дешевле, Pro дороже. Нет больших первоначальных затрат, но при высоких объёмах токенов — значительные расходы. |
| **Простота развёртывания** | Сложно: нужен DevOps/ML-опыт, настройка фреймворка, установка моделей, оптимизация (квантование, inference-библиотеки). Дообучение на документации — дополнительная работа. Полный контроль над данными и моделью. | Очень просто: API готово к использованию. Нет необходимости в серверах, масштабированием и обновлениями занимается OpenAI. Интеграция через HTTP/SDK. | Достаточно просто: API Yandex Cloud, веб-консоль для управления, выбор Lite/Pro. Минимальные усилия на интеграцию, но в рамках экосистемы Yandex Cloud. |

## Основные выводы

1. **Качество vs Контроль:**  
   - GPT-4 (OpenAI) обычно даёт лучшие ответы на английском языке.  
   - Локальные модели Hugging Face можно дообучить на своих документах, чтобы повысить точность, но требуется работа ML-инженеров.  
   - YandexGPT сильна на русском, английский уступает GPT-4.

2. **Скорость:**  
   - На локальных серверах минимальная задержка при мощной инфраструктуре.  
   - Облачные решения стабильно 1–3 секунды, Lite-модели YandexGPT быстрее для простых запросов.

3. **Стоимость:**  
   - Локальные модели — высокие начальные и постоянные расходы (CapEx + OpEx).  
   - Облачные решения — оплата по использованию (OPEX), дешевле для пилотных и средних внедрений.  

4. **Развёртывание:**  
   - Облако проще и быстрее интегрировать, поддержка, масштабирование и обновления — за провайдером.  
   - On-prem — сложнее, но полный контроль над данными, что важно для корпоративной безопасности.


# 2.Сравнение эмбеддинг-моделей для RAG (английский язык)

Сравнение Sentence-Transformers (локально и в облаке) и OpenAI Embeddings (text-embedding-ada-002, text-embedding-3-small/large) по скорости, качеству и стоимости.

| Критерий                           | Sentence-Transformers (локально, CPU/GPU)                                                                              | Sentence-Transformers (облачное API)                                            | OpenAI text-embedding-ada-002                                                      | OpenAI text-embedding-3-small                                                       | OpenAI text-embedding-3-large                                                       |
|------------------------------------|------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------|-------------------------------------------------------------------------------|-------------------------------------------------------------------------------|-------------------------------------------------------------------------------|
| **Скорость создания индекса**      | Очень высокая при GPU (тысячи–десятки тысяч текстов/сек). На CPU – сотни текстов/сек. Нет сетевых задержек.           | Зависит от провайдера. API-вызовы медленнее локальной генерации, есть сетевые задержки. | Эмбеддинг 42 000 документов ≈9 мин (~75 документов/сек)                        | Аналогично ada-002, небольшое замедление при большем размере эмбеддинга        | Дольше ada-002, вектор 3072D, больше памяти и времени на генерацию             |
| **Качество поиска**                | Высокое. `all-mpnet-base-v2` – лучшая точность, `multi-qa-mpnet-base-dot-v1` оптимизирована для semantic search.       | Зависит от выбранной модели, то же качество, что локально.                        | Среднее, MTEB ≈61.0                                                          | Выше, чем ada-002, MTEB ≈62.3                                               | Наилучшее качество OpenAI, MTEB ≈64.6                                         |
| **Стоимость (API/инфраструктура)** | Нет платы за запросы, но нужны серверы/GPU (~\$0.5–3/час). Выгодно при больших объёмах.                               | Требуется облачный сервис (≈\$0.03–0.1/час). Удобно для небольших нагрузок.       | \$0.0001 за 1K токенов (~\$52/мес на 10K документов)                          | \$0.00002 за 1K токенов (~\$1.6/мес на 10K документов)                        | \$0.00013 за 1K токенов (~\$10.4/мес на 10K документов)                        |
| **Плюсы / Минусы**                 | **Плюсы:** полный контроль, конфиденциальность, гибкость, можно дообучать. **Минусы:** нужны ресурсы, опыт, настройка. | **Плюсы:** простота API, не нужны серверы, то же качество, что локально. **Минусы:** задержка сети, зависимость от провайдера, доп. расходы. | **Плюсы:** простота API, быстрый прототип. **Минусы:** устаревшая точность, платно, ограничения токенов. | **Плюсы:** низкая цена, лучше ada-002, экономичное качество. **Минусы:** чуть хуже 3-large, 1536D. | **Плюсы:** наилучшее качество, поддержка длинного контекста. **Минусы:** выше цена, 3072D, больше памяти. |

## Выводы

- **Sentence-Transformers локально**: выгодны при больших объёмах, полный контроль, высокая скорость на GPU.  
- **Sentence-Transformers облачно**: удобны для небольших проектов, быстро интегрируются, но дороже при масштабах.  
- **OpenAI Embeddings**: простая интеграция, новые модели (3-small/3-large) лучше по качеству, цена зависит от модели.

# 3.Сравнение векторных баз ChromaDB и FAISS для RAG (английский язык)

Сравнение с учётом локального и облачного развёртывания, объёмов в тысячи документов и необходимости фильтрации по метаданным (роль пользователя и тип документа).

| Критерий                         | ChromaDB                                                                                                   | FAISS                                                                                          |
|----------------------------------|-----------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------|
| **Скорость поиска и индексации** | Поиск: несколько мс (≈2.5 мс на запрос). Индексация: быстро для малых объёмов, несколько минут для 1 млн векторов. GPU нет. | Поиск: субмиллисекундный (≈0.3 мс), GPU ускоряет ещё сильнее. Индексация: быстрее, например 1 млн векторов ≈2 мин. Отлично масштабируется. |
| **Сложность внедрения и поддержки** | Простое развёртывание: API, SDK, встроенная поддержка метаданных и бэкапов. Настройка занимает минуты. | Требуется разработка API поверх библиотеки, настройка индексов, сохранение, бэкап – сложнее. Нужно больше инженерных ресурсов. |
| **Удобство в работе**             | Простой Python/JS API, поддержка фильтрации по метаданным и базовый полнотекстовый поиск, интеграции с RAG «из коробки». | Низкоуровневая библиотека: фильтрация метаданных и API нужно реализовывать самостоятельно. Поддержка интеграции зависит от разработчика. |
| **Стоимость владения**            | Open-source, можно запускать на обычных серверах CPU. Облачный Chroma Cloud – платно по операциям. Лёгкая инфраструктура для тысяч документов. | Open-source, можно запускать локально или на облаке, основные расходы – сервер/VM/GPU. Для малых объёмов достаточно CPU. Требует больше усилий разработчиков. |

## Выводы

- **ChromaDB**: удобнее для прототипов, быстрый старт, есть поддержка метаданных и фильтрации, минимальные усилия по инфраструктуре.  
- **FAISS**: оптимален при высоких нагрузках и больших объёмах, субмиллисекундная скорость поиска, высокая гибкость настройки индексов, но сложнее интегрировать и поддерживать.  
- Для наших задач (тысячи документов, он-премис, фильтрация по ролям и типам документов) ChromaDB обеспечивает удобство внедрения и достаточную производительность, FAISS – стратегический выбор для масштабирования и максимальной скорости.

# 4.Выберите рекомендуемую конфигурацию сервера (CPU, RAM, GPU), чтобы развернуть RAG-бота

**Рекомендуемая конфигурация**:	 OpenAI ChatGPT (облако) + text-embedding-3-small + ChromaDB.

- _Простота внедрения:_ ChromaDB + OpenAI SDK позволяют быстро настроить прототип без сложной инженерной работы. 
- _Поддержка фильтров и метаданных_: ChromaDB позволяет сразу фильтровать по типу документа и роли пользователя, что критично для корпоративного поиска. 
- _Стоимость_: text-embedding-3-small + ChromaDB — очень экономично при малых объёмах. 
- _Риски_: зависимость от облака OpenAI и скорость поиска ChromaDB (хотя для тысяч документов это не проблема).
 


| Вариант | LLM | Эмбеддинги | Векторная база | Качество поиска | Скорость поиска | Удобство внедрения | Метаданные/фильтрация | Стоимость владения | Комментарий                                                                       |
|---------|-----|------------|----------------|----------------|----------------|------------------|----------------------|------------------|-----------------------------------------------------------------------------------|
| 1 | OpenAI ChatGPT (облако) | text-embedding-3-small | ChromaDB | Хорошее, достаточное для тысяч документов | Несколько мс на запрос | Очень просто: Python SDK, REST API | Есть встроенная поддержка фильтров | Низкая при малых объёмах | Быстрый старт, простота, подход для пилота                                        |
| 2 | OpenAI ChatGPT (облако) | text-embedding-3-large | FAISS | Отличное качество, максимум семантики | Субмиллисекундный на GPU | Сложнее: нужно разрабатывать API для метаданных | Нужно реализовывать самостоятельно | Выше из-за GPU и разработки | Подходит для  высокой нагрузки и точным поиском                                   |
| 3 | Локальный LLM (GPT-J / Mistral) | ST all-MiniLM-L6-v2 | ChromaDB | Среднее качество LLM и эмбеддингов | Быстро для малых объёмов, CPU | Настройка сложнее, требует GPU/сервер | Есть, встроено в ChromaDB | Низкая (нет API) | Хорошо для полного контроля и приватности данных, но LLM слабее облачного ChatGPT |
| 4 | OpenAI ChatGPT (облако) | Комбинация ST + OpenAI small | FAISS | Хорошее + точное для сложных запросов | Быстро на FAISS, комбинированная логика | Сложнее интеграции (двойной слой эмбеддингов) | Нужно реализовать самостоятельно | Средняя/высокая, зависит от инфраструктуры | Баланс скорости и точности, подходит для роста и масштабирования                  |

#  4. Рекомендации по конфигурации сервера для RAG-бота

Вариант: _OpenAI ChatGPT (облако) + sentence-transformers/all-MiniLM-L6-v2 + ChromaDB_  
Объём данных: тысячи документов

### 1. Минимальная конфигурация (Пилот / PoC)

| Компонент | Рекомендация | Обоснование |
|-----------|--------------|------------|
| CPU       | 4 ядра (x86 или ARM, 2.5 GHz+) | Достаточно для обработки запросов, построения индекса ChromaDB и параллельных API-вызовов |
| RAM       | 16 GB | Хватает для хранения индекса и работы сервиса Python/Flask/FastAPI |
| Storage   | 100 GB SSD | Для хранения документов, индексов и логов; SSD ускоряет доступ |
| GPU       | Не нужен | Embeddings и LLM работают в облаке, локально нагрузка лёгкая |
| Network   | 100 Mbps+ | Для стабильного соединения с OpenAI API |

### 2. Рекомендованная конфигурация (Продакшен с ростом)

| Компонент | Рекомендация | Обоснование |
|-----------|--------------|------------|
| CPU       | 8–16 ядер | Для обработки большого числа запросов параллельно и индексации при росте документации |
| RAM       | 32–64 GB | Для хранения всех векторов и метаданных в памяти ChromaDB, минимизация I/O |
| Storage   | 500 GB SSD | Для всех документов, резервных копий и логов |
| GPU       | Не обязателен (опционально для будущих локальных LLM) | Можно ускорить обработку больших индексов и локальных моделей |
| Network   | 1 Gbps | Для стабильной работы с облачными LLM и OpenAI API |
